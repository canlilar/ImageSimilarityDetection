{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image==0.19.2 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.19.2)\n",
      "Requirement already satisfied: pandas==1.4.2 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from scikit-image==0.19.2->-r requirements.txt (line 1)) (1.22.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from scikit-image==0.19.2->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from scikit-image==0.19.2->-r requirements.txt (line 1)) (2022.5.4)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from scikit-image==0.19.2->-r requirements.txt (line 1)) (2.19.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from scikit-image==0.19.2->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from scikit-image==0.19.2->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from scikit-image==0.19.2->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from scikit-image==0.19.2->-r requirements.txt (line 1)) (9.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from pandas==1.4.2->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from pandas==1.4.2->-r requirements.txt (line 2)) (2022.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from packaging>=20.0->scikit-image==0.19.2->-r requirements.txt (line 1)) (3.0.8)\n",
      "Requirement already satisfied: six>=1.5 in /Users/canlilareden/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas==1.4.2->-r requirements.txt (line 2)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Use\n",
    "import csv\n",
    "import sys\n",
    "#import requests\n",
    "import skimage.io\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from IPython.display import display, Image, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import cv2\n",
    "import h5py\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import keras\n",
    "\n",
    "# TensorFlow\n",
    "#import tensorflow as tf\n",
    "\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras imports\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.xception import Xception\n",
    "# from keras.applications.resnet50 import ResNet50 \n",
    "# from keras_applications.resnet import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, BatchNormalization\n",
    "# from keras.utils import multi_gpu_model\n",
    "from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file points to the Watchfinder dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] start time - 2022-05-06 12:47\n"
     ]
    }
   ],
   "source": [
    "# load the user configs\n",
    "with open('conf/conf.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# config variables\n",
    "model_name    = config[\"model\"]\n",
    "weights     = config[\"weights\"]\n",
    "include_top   = config[\"include_top\"]\n",
    "train_path    = config[\"train_path\"]\n",
    "features_path   = config[\"features_path\"]\n",
    "labels_path   = config[\"labels_path\"]\n",
    "test_size     = config[\"test_size\"]\n",
    "results     = config[\"results\"]\n",
    "model_path    = config[\"model_path\"]\n",
    "\n",
    "# start time\n",
    "print (\"[STATUS] start time - {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")))\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a pre-trained network\n",
    "In this case we pick VGG16: it is simple and fast enough for the dataset in use.  \n",
    "We cut it at the layer before the final classifier and create our own model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights=weights)\n",
    "# model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "model = Model()\n",
    "image_size = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode labels\n",
    "We run the pictures through the network and associate each feature set to its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] encoding labels...\n",
      "[INFO] completed label - .DS_Store\n",
      "[INFO] completed label - Child1 copy.bmp\n",
      "[INFO] completed label - Father1 copy.bmp\n",
      "[INFO] completed label - Mother1 copy.bmp\n",
      "[STATUS] training labels: []\n",
      "[STATUS] training labels shape: (0,)\n"
     ]
    }
   ],
   "source": [
    "# path to training dataset\n",
    "train_labels = os.listdir(train_path)\n",
    "\n",
    "# encode the labels\n",
    "print (\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "le.fit([tl for tl in train_labels])\n",
    "\n",
    "# variables to hold features and labels\n",
    "features = []\n",
    "labels   = []\n",
    "file_map=[]\n",
    "\n",
    "# loop over all the labels in the folder\n",
    "count = 1\n",
    "for i, label in enumerate(train_labels):\n",
    "  cur_path = train_path + \"/\" + label\n",
    "  count = 1\n",
    "  for image_path in glob.glob(cur_path + \"/*.jpg\"):\n",
    "    try:\n",
    "      img = image.load_img(image_path, target_size=image_size)\n",
    "      x = image.img_to_array(img)\n",
    "      x = np.expand_dims(x, axis=0)\n",
    "      x = preprocess_input(x)\n",
    "      # extract features  \n",
    "      feature = model.predict(x)\n",
    "      flat = feature.flatten()\n",
    "      features.append(flat)\n",
    "      labels.append(label)\n",
    "      file_map.append(image_path)  \n",
    "      print (\"[INFO] processed - \" + str(count))\n",
    "      count += 1\n",
    "    except:\n",
    "      pass\n",
    "  print (\"[INFO] completed label - \" + label)\n",
    "\n",
    "# encode the labels using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le_labels = le.fit_transform(labels)\n",
    "\n",
    "# get the shape of training labels\n",
    "print (\"[STATUS] training labels: {}\".format(le_labels))\n",
    "print (\"[STATUS] training labels shape: {}\".format(le_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/venvs/image-sim-venv2/lib/python3.8/site-packages/numpy/core/shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m arrs \u001b[38;5;241m=\u001b[39m atleast_2d(\u001b[38;5;241m*\u001b[39mtup)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 282\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "features=np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distances\n",
    "We'll pick a query image, then compute cosine, euclidean and Hamming distances among it and the rest of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=1001\n",
    "print(file_map[idx])\n",
    "Image(file_map[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "distances = pairwise_distances(features[idx,:].reshape(1,-1), features, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the 9 closest images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=np.argsort(distances)[0][:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Display_Images(images, header=None, width=\"100%\"): # to match Image syntax\n",
    "    if type(width)==type(1): width = \"{}px\".format(width)\n",
    "    html = [\"<table style='width:{}'><tr>\".format(width)]\n",
    "    if header is not None:\n",
    "        html += [\"<th>{}</th>\".format(h) for h in header] + [\"</tr><tr>\"]\n",
    "\n",
    "    cols=1\n",
    "    for image in images:\n",
    "        print(image)\n",
    "        html.append(\"<td><img src='{}' /></td>\".format(image))\n",
    "        cols+=1\n",
    "        if (cols>3):\n",
    "            html.append(\"</tr><tr>\")\n",
    "            cols=1\n",
    "    html.append(\"</tr></table>\")\n",
    "    display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Display_Images([file_map[i] for i in indices],width=\"100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(features[idx,:].reshape(1,-1), features, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=np.argsort(distances)[0][:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Display_Images([file_map[i] for i in indices],width=\"100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdistances = pairwise_distances(features[idx,:].reshape(1,-1), features, metric='hamming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdistances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindices=np.argsort(hdistances)[0][:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Display_Images([file_map[i] for i in hindices],width=\"100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a binary code explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "print(scaler.fit(features))\n",
    "scaled_features=scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features = (scaled_features<0.5).astype(int)\n",
    "print(binary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_query=scaler.transform(features[idx,:].reshape(1,-1))\n",
    "binary_query=(scaled_query<0.5).astype(int)\n",
    "print(binary_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdistances2 = pairwise_distances(binary_query, binary_features, metric='hamming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hdistances2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindices=np.argsort(hdistances)[0][:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Display_Images([file_map[i] for i in hindices],width=\"100%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Hashing\n",
    "Now let us try with the deep fashion dataset http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html  \n",
    "We will add a layer for 64-bit semantic hashing, i.e. to characterize each image with a 64-bit number.  \n",
    "There is no \"binary\" activation function, so we'll approximate with tanh and then do some postprocessing.  \n",
    "64 bit is current limitation of MySQL bit() type on mysql5.7, which we want to use in azure to offload search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files Preprocessing\n",
    "We move the data into directories reflecting the 46 categories, then split into train and test.  \n",
    "Pretty boring - not much to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#prepare directory structures\n",
    "# path to training dataset\n",
    "img_path='./fashion_data/img'\n",
    "train_path='./fashion_data/imgtrain'\n",
    "\n",
    "# Copy images into fundamental categories structure\n",
    "import os\n",
    "source = img_path\n",
    "dest = train_path\n",
    "dirs = os.listdir(source)\n",
    "import shutil\n",
    "import numpy as np\n",
    "for d in dirs:\n",
    "    files=os.listdir(os.path.join(source,d))\n",
    "    #print (len(files))\n",
    "    #if len(files)>50:\n",
    "        \n",
    "    newdir = d.rsplit('_',1)[1]\n",
    "    print (d.rsplit('_',1)[1])\n",
    "    newdest=dest+'/'+newdir\n",
    "    if not os.path.exists(newdest):\n",
    "                os.mkdir(newdest)   \n",
    "\n",
    "    for f in files:\n",
    "        #try:\n",
    "            full_file_name = os.path.join(d, f)\n",
    "\n",
    "            target_file_name=os.path.join(newdest,f)\n",
    "            if os.path.exists(target_file_name):\n",
    "                target_file_name = os.path.splitext(target_file_name)[0]+'_'+time.strftime(\"%Y%m%d-%H%M%S\")+os.path.splitext(target_file_name)[1]\n",
    "\n",
    "            print(source+'/'+full_file_name)\n",
    "            print(target_file_name)\n",
    "\n",
    "            shutil.copy(source+'/'+full_file_name, target_file_name)\n",
    "        #except:\n",
    "        #    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move some images to test directory\n",
    "# path to training dataset\n",
    "from math import floor\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "train_path='./fashion_data/imgtrain'\n",
    "test_path='./fashion_data/imgtest'\n",
    "\n",
    "#move some images for testing\n",
    "source = train_path\n",
    "dest = test_path\n",
    "dirs = os.listdir(source)\n",
    "count=0\n",
    "for d in dirs:\n",
    "    print(d)\n",
    "    count +=1\n",
    "    print(count)\n",
    "    if ('.DS_Store' not in d):\n",
    "        files=os.listdir(os.path.join(source,d))\n",
    "        #select 10% random items        \n",
    "        for f in random.sample(files,floor(len(files)*0.1)):\n",
    "            #try:\n",
    "\n",
    "                full_file_name = os.path.join(d, f)\n",
    "                print (full_file_name)\n",
    "                if not os.path.exists(dest+'/'+d):\n",
    "                    os.mkdir(dest+'/'+d)\n",
    "                print(source+'/'+full_file_name)\n",
    "                print(dest+'/'+full_file_name)\n",
    "                #fore some reason move does not work - it causes a filename does not exist...\n",
    "                shutil.move(source+'/'+full_file_name, dest+'/'+full_file_name)                \n",
    "            #except:\n",
    "            #    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets for Model\n",
    "We use the ImageDataGenerator class to create a training, test and validation dataset by iterating over our directory structure. Each directory name is a label.  \n",
    "We also take advantage of some of the image processing features to introduce some \"randomness\" to make the classification more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_path='./fashion_data/img/train'\n",
    "test_path='./fashion_data/img/val'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30.,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    "    #samplewise_center=True,\n",
    "    #samplewise_std_normalization=True\n",
    "        #rescale=1./255\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    #samplewise_center=True,\n",
    "    #samplewise_std_normalization=True\n",
    "    #rescale=1./255\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping,ReduceLROnPlateau\n",
    "from keras.layers import Activation, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD, Adam, rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let us try with deep fashion dataset and add a layer for 64-bit semantic hashing\n",
    "# 64 bit is current limitation of MySQL bit() type on mysql5.7, which we want to use in azure to offload search\n",
    "#base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "#base_model.summary()\n",
    "#image_size = (299, 299)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "#x = base_model.output\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "base_model = NASNetMobile(weights='imagenet', include_top=False, pooling='avg', input_shape=(224,224,3))\n",
    "#encoder=Flatten()(model.output)\n",
    "#x=base_model.get_layer(base_model.layers[-2].name).output\n",
    "#for layer in base_model.layers:\n",
    "#    layer.trainable=False\n",
    "x=base_model.output\n",
    "#x=Flatten()(x)\n",
    "#x=Dense(256,activation='relu',name='encoder')(x)\n",
    "#x=Dropout(0.3)(x)\n",
    "#x=Dense(256,activation='relu')(x)\n",
    "x=Dense(64, name='hash')(x)\n",
    "#x=BatchNormalization()(x)\n",
    "x=Activation('hard_sigmoid',name='hash-out')(x)\n",
    "#x=Dense(2048, name='decoder', activation='relu')(x)\n",
    "predictions=Dense(46, activation='softmax')(x)\n",
    "\n",
    "#for layer in base_model.layers[:-12]:\n",
    "#    layer.trainable=False\n",
    "\n",
    "#create graph of your new model\n",
    "G=1\n",
    "if G>1:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        model = Model(input = base_model.input, output = predictions)\n",
    "    print(\"Training with {} GPUs\".format(G))\n",
    "    model=multi_gpu_model(model, gpus=G)\n",
    "else:\n",
    "    model = Model(input = base_model.input, output = predictions)\n",
    "\n",
    "#compile the model\n",
    "#sgd=SGD(lr=0.0001, momentum=0.5, nesterov=False)\n",
    "adam=Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as of version 1.7 tensorboard consumes gpu. we don't want that\n",
    "tb=TensorBoard()\n",
    "tb.set_model(model)\n",
    "tb.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of tricks to control the gradient descent\n",
    "# reduce the learning rate on plateaus\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_acc',\n",
    "                               patience=5,\n",
    "                               factor=0.2,\n",
    "                               cooldown=1,\n",
    "                               verbose=1)\n",
    "#tensorboard = TensorBoard(log_dir='./logs')\n",
    "# stop if valuation accuracy plateaus \n",
    "early_stopper = EarlyStopping(monitor='val_acc',\n",
    "                              patience=11,\n",
    "                              verbose=1)\n",
    "# save the model at every improvement\n",
    "checkpoint = ModelCheckpoint(\"Resnet50_encoder2048_hash64_5slow.h5\", \n",
    "                             monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, save_weights_only=False, \n",
    "                             mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_generator), len(validation_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "For time's sake, we can reduce the number of steps per epoch, but that is going to cost us in valuation accuracy as we're going to leave a subset of pictures \"unseen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=2200,\n",
    "    epochs=200,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=420,\n",
    "    callbacks=[ #lr_reducer,\n",
    "               early_stopper, \n",
    "               checkpoint\n",
    "               #tb\n",
    "              ],\n",
    "    workers=4,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('resnet50_hash64.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test \n",
    "We test on a different subset and compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator()\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        \"./fashion_data/img/test\",\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print (model.metrics_names)\n",
    "print (scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize\n",
    "We calculate the hash codes of our catalog by running the pictures through the tuned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_model=Model(input=model.input, output=model.get_layer('hash-out').output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hash_model.predict_generator(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path to training dataset\n",
    "train_path='./fashion_data/img/test'\n",
    "train_labels = os.listdir(train_path)\n",
    "image_size = (224, 224)\n",
    "print (\"[INFO] encoding image hashes...\")\n",
    "\n",
    "def binarize (a_list):\n",
    "    b=''.join(str(e) for e in a_list)\n",
    "    return int(b,2)\n",
    "\n",
    "# variables to hold features and labels\n",
    "\n",
    "features = []\n",
    "labels   = []\n",
    "file_map=[]\n",
    "table=pd.DataFrame(columns=['id', 'path', 'label', 'code'])\n",
    "                            \n",
    "# loop over all the labels in the folder\n",
    "count = 1\n",
    "for i, label in enumerate(train_labels):\n",
    "    cur_path = train_path + \"/\" + label \n",
    "    print(cur_path)\n",
    "    for image_path in glob.glob('{}/**/*.jpg'.format(cur_path), recursive=True):\n",
    "        #try:\n",
    "            img = image.load_img(image_path, target_size=image_size)\n",
    "            x = image.img_to_array(img)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            x = preprocess_input(x)\n",
    "            #generate hash using hash model\n",
    "            hash_out = hash_model.predict(x)\n",
    "            # a very crude way to generate a binary number, but effective\n",
    "            bin_out = np.where(hash_out.flatten()>0.5,1,0)           \n",
    "            features.append(bin_out)  \n",
    "            labels.append(label)\n",
    "            file_map.append(image_path)\n",
    "            num_out=binarize(bin_out)\n",
    "            print (image_path, num_out)\n",
    "            table=table.append(pd.DataFrame(data=[[count, image_path, label, num_out]], columns=['id', 'path', 'label', 'code']), ignore_index=True)\n",
    "            print (\"[INFO] processed - \" + str(count))\n",
    "            count += 1\n",
    "        #except:\n",
    "        #    pass\n",
    "    print (\"[INFO] completed label - \" + label)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "# Obtain connection string information from the portal\n",
    "config = {\n",
    "  'host':'gmmysql.mysql.database.azure.com',\n",
    "  'user':'gmarchetti@gmmysql',\n",
    "  'password':'ScienceGuy1!',\n",
    "  'database':'vsdb'\n",
    "}\n",
    "\n",
    "\n",
    "import sqlalchemy\n",
    "database_username = 'gmarchetti@gmmysql'\n",
    "database_password = 'ScienceGuy1!'\n",
    "database_ip       = 'gmmysql.mysql.database.azure.com'\n",
    "database_name     = 'vsdb'\n",
    "database_connection = sqlalchemy.create_engine('mysql+mysqlconnector://{0}:{1}@{2}/{3}'.\n",
    "                                               format(database_username, database_password, \n",
    "                                                      database_ip, database_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.to_sql(con=database_connection, name='imagedata', if_exists='replace')    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query to select top 9 with hamming distance from given hash\n",
    "\n",
    "from sqlalchemy import text\n",
    "database_connection = sqlalchemy.create_engine('mysql+mysqlconnector://{0}:{1}@{2}/{3}'.\n",
    "                                               format(database_username, database_password, \n",
    "                                                      database_ip, database_name))\n",
    "\n",
    "sql = text('SELECT id, path, label, BIT_COUNT(105117672137245588 ^ code)  as hd FROM imagedata ORDER BY hd ASC limit 9;')\n",
    "result = database_connection.engine.execute(sql)\n",
    "print(result)\n",
    "\n",
    "file_map=[]\n",
    "for row in result:\n",
    "    print (row)\n",
    "    file_map.append(row[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We picked a blazer. The catalog contains several categories. So far, the algorithm has at least identified that it is a blazer. However, you can see that several have distance 0. Our 64-bit binarization algorithm loses quite a bit of resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Display_Images(file_map,width=\"100%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only tanks, just to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query to select top 9 with hamming distance from given hash\n",
    "\n",
    "from sqlalchemy import text\n",
    "database_connection = sqlalchemy.create_engine('mysql+mysqlconnector://{0}:{1}@{2}/{3}'.\n",
    "                                               format(database_username, database_password, \n",
    "                                                      database_ip, database_name))\n",
    "\n",
    "sql = text('SELECT id, path, label, BIT_COUNT(105117109492337556 ^ code)  as hd FROM imagedata WHERE label=\\'Tank\\' ORDER BY hd ASC limit 9 ;')\n",
    "result = database_connection.engine.execute(sql)\n",
    "print(result)\n",
    "\n",
    "file_map=[]\n",
    "for row in result:\n",
    "    print (row)\n",
    "    file_map.append(row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Display_Images(file_map,width=\"100%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train  with 128 bit resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_path='./fashion_data/img/train'\n",
    "test_path='./fashion_data/img/val'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30.,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    "    #samplewise_center=True,\n",
    "    #samplewise_std_normalization=True\n",
    "        #rescale=1./255\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    #samplewise_center=True,\n",
    "    #samplewise_std_normalization=True\n",
    "    #rescale=1./255\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let us try with deep fashion dataset and add a layer for 64-bit semantic hashing\n",
    "# 64 bit is current limitation of MySQL bit() type on mysql5.7, which we want to use in azure to offload search\n",
    "#base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "#base_model.summary()\n",
    "#image_size = (299, 299)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "#x = base_model.output\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(224,224,3))\n",
    "#encoder=Flatten()(model.output)\n",
    "x=base_model.get_layer(base_model.layers[-2].name).output\n",
    "#x=Flatten()(x)\n",
    "#x=Dense(256,activation='relu',name='encoder')(x)\n",
    "#x=Dropout(0.3)(x)\n",
    "#x=Dense(256,activation='relu')(x)\n",
    "x=Dense(1024, name='hash')(x)\n",
    "#x=BatchNormalization()(x)\n",
    "x=Activation('hard_sigmoid',name='hash-out')(x)\n",
    "#x=Dense(2048, name='decoder', activation='relu')(x)\n",
    "predictions=Dense(46, activation='softmax')(x)\n",
    "\n",
    "#for layer in base_model.layers[:-12]:\n",
    "#    layer.trainable=False\n",
    "\n",
    "#create graph of your new model\n",
    "G=1\n",
    "if G>1:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        model2 = Model(input = base_model.input, output = predictions)\n",
    "    print(\"Training with {} GPUs\".format(G))\n",
    "    model2=multi_gpu_model(model, gpus=G)\n",
    "else:\n",
    "    model2 = Model(input = base_model.input, output = predictions)\n",
    "\n",
    "#compile the model\n",
    "#sgd=SGD(lr=0.0001, momentum=0.5, nesterov=False)\n",
    "adam=Adam(lr=0.001)\n",
    "model2.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model2.summary()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of tricks to control the gradient descent\n",
    "# reduce the learning rate on plateaus\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_acc',\n",
    "                               patience=5,\n",
    "                               factor=0.2,\n",
    "                               cooldown=1,\n",
    "                               verbose=1)\n",
    "#tensorboard = TensorBoard(log_dir='./logs')\n",
    "# stop if valuation accuracy plateaus \n",
    "early_stopper = EarlyStopping(monitor='val_acc',\n",
    "                              patience=11,\n",
    "                              verbose=1)\n",
    "# save the model at every improvement\n",
    "checkpoint = ModelCheckpoint(\"Resnet50_encoder2048_hash1024_adam.h5\", \n",
    "                             monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, save_weights_only=False, \n",
    "                             mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_generator), len(validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history=model2.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=2200,\n",
    "    epochs=200,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=420,\n",
    "    callbacks=[ #lr_reducer,\n",
    "               early_stopper, \n",
    "               checkpoint\n",
    "               #tb\n",
    "              ],\n",
    "    workers=12,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
